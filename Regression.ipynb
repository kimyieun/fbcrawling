{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1988, 16)\n",
      "(1988, 1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "try:\n",
    "    with open(\"./Features/Features_data/featuretest.json\", encoding='UTF8') as f:\n",
    "        features = json.load(f)\n",
    "except EnvironmentError:\n",
    "    print('No Feature File')\n",
    "\n",
    "try:\n",
    "    with open(\"./Features/Features_data/scoretest.json\", encoding='UTF8') as f:\n",
    "        scores = json.load(f)\n",
    "except EnvironmentError:\n",
    "    print(\"No Score File\")\n",
    "\n",
    "feature_num = 16\n",
    "x = np.array(features)\n",
    "print(x.shape)\n",
    "\n",
    "y = np.array(scores)\n",
    "y = np.reshape(y,(-1,1))\n",
    "#y = torch.unsqueeze(y, dim = 1)\n",
    "print(y.shape)\n",
    "kf = KFold(n_splits = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, 10)   # hidden layer\n",
    "        #self.hidden3 = torch.nn.Linear(n_hidden, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(10, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = F.relu(self.hidden2(x))      # activation function for hidden layer\n",
    "        #x = F.relu(self.hidden3(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-6.4622, grad_fn=<AddBackward>)\n",
      "13.400835\n",
      "tensor(0.4360, grad_fn=<AddBackward>)\n",
      "1.0127804\n",
      "tensor(0.4670, grad_fn=<AddBackward>)\n",
      "0.9571339\n"
     ]
    }
   ],
   "source": [
    "X_train = Variable(torch.from_numpy(x).float())\n",
    "y_train = Variable(torch.from_numpy(y).float())\n",
    "\n",
    "net = Net(n_feature=feature_num, n_hidden=50, n_output=1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for t in range(600):\n",
    "    y_train_pred = net(X_train)\n",
    "    loss = loss_func(y_train_pred, y_train)     # must be (1. nn output, 2. target)\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    y_train_bar = y_train.mean()\n",
    "    sse_train = ((y_train - y_train_pred)**2).sum()\n",
    "    sst_train = ((y_train - y_train_bar)**2).sum()\n",
    "    train_r2 = 1 - sse_train/sst_train \n",
    "    \n",
    "    if t % 200 == 0:\n",
    "        print(train_r2)\n",
    "        print(loss.data.numpy())\n",
    "\n",
    "torch.save(net.state_dict(), './model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden): Linear(in_features=16, out_features=50, bias=True)\n",
      "  (hidden2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-2.8484, grad_fn=<AddBackward>) tensor(-1.0701, grad_fn=<AddBackward>)\n",
      "7.0405955\n",
      "[[0.2306937 ]\n",
      " [0.13228501]\n",
      " [0.18185365]\n",
      " ...\n",
      " [1.4976921 ]\n",
      " [0.268609  ]\n",
      " [0.3141815 ]]\n",
      "tensor(0.4491, grad_fn=<AddBackward>) tensor(0.4029, grad_fn=<AddBackward>)\n",
      "1.0077943\n",
      "[[1.5680958]\n",
      " [1.6657923]\n",
      " [1.0915687]\n",
      " ...\n",
      " [3.7619007]\n",
      " [2.5717046]\n",
      " [2.5803719]]\n",
      "tensor(0.4681, grad_fn=<AddBackward>) tensor(0.4181, grad_fn=<AddBackward>)\n",
      "0.97309065\n",
      "[[1.6075637]\n",
      " [1.7209147]\n",
      " [1.1574222]\n",
      " ...\n",
      " [4.103382 ]\n",
      " [2.6340718]\n",
      " [2.5969589]]\n",
      "Net(\n",
      "  (hidden): Linear(in_features=16, out_features=50, bias=True)\n",
      "  (hidden2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-9.3631, grad_fn=<AddBackward>) tensor(-1.7608, grad_fn=<AddBackward>)\n",
      "18.404816\n",
      "[[-0.01741424]\n",
      " [-0.5676571 ]\n",
      " [ 0.20220661]\n",
      " ...\n",
      " [-4.328232  ]\n",
      " [ 0.10932921]\n",
      " [-0.1790202 ]]\n",
      "tensor(0.4383, grad_fn=<AddBackward>) tensor(0.4395, grad_fn=<AddBackward>)\n",
      "0.9975252\n",
      "[[1.684103 ]\n",
      " [2.627443 ]\n",
      " [1.6391311]\n",
      " ...\n",
      " [3.9205074]\n",
      " [2.778839 ]\n",
      " [2.7769413]]\n",
      "tensor(0.4689, grad_fn=<AddBackward>) tensor(0.4330, grad_fn=<AddBackward>)\n",
      "0.94328576\n",
      "[[1.841038 ]\n",
      " [2.7599313]\n",
      " [1.7234633]\n",
      " ...\n",
      " [4.0214434]\n",
      " [2.5279384]\n",
      " [2.8090637]]\n",
      "Net(\n",
      "  (hidden): Linear(in_features=16, out_features=50, bias=True)\n",
      "  (hidden2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-9.4143, grad_fn=<AddBackward>) tensor(-1.7706, grad_fn=<AddBackward>)\n",
      "18.85349\n",
      "[[-0.02097028]\n",
      " [-0.52598584]\n",
      " [ 0.2955856 ]\n",
      " ...\n",
      " [-4.6926646 ]\n",
      " [ 0.3055374 ]\n",
      " [-0.06424344]]\n",
      "tensor(0.4479, grad_fn=<AddBackward>) tensor(0.3381, grad_fn=<AddBackward>)\n",
      "0.99946654\n",
      "[[1.750358 ]\n",
      " [2.6644597]\n",
      " [1.5090714]\n",
      " ...\n",
      " [3.9962583]\n",
      " [2.7495909]\n",
      " [2.7606115]]\n",
      "tensor(0.4614, grad_fn=<AddBackward>) tensor(0.3347, grad_fn=<AddBackward>)\n",
      "0.9751206\n",
      "[[1.7663009]\n",
      " [2.703126 ]\n",
      " [1.6535408]\n",
      " ...\n",
      " [4.1169057]\n",
      " [2.579301 ]\n",
      " [2.704648 ]]\n",
      "Net(\n",
      "  (hidden): Linear(in_features=16, out_features=50, bias=True)\n",
      "  (hidden2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-3.7960, grad_fn=<AddBackward>) tensor(-0.9960, grad_fn=<AddBackward>)\n",
      "8.514684\n",
      "[[ 0.09881037]\n",
      " [-0.07270264]\n",
      " [ 1.527961  ]\n",
      " ...\n",
      " [ 0.663799  ]\n",
      " [ 1.0433626 ]\n",
      " [ 0.02046207]]\n",
      "tensor(0.4400, grad_fn=<AddBackward>) tensor(0.4333, grad_fn=<AddBackward>)\n",
      "0.99419713\n",
      "[[2.6674979]\n",
      " [1.1331283]\n",
      " [2.1498435]\n",
      " ...\n",
      " [3.71127  ]\n",
      " [3.8425357]\n",
      " [2.7527099]]\n",
      "tensor(0.4628, grad_fn=<AddBackward>) tensor(0.4418, grad_fn=<AddBackward>)\n",
      "0.95379543\n",
      "[[2.786008 ]\n",
      " [1.3166327]\n",
      " [2.132702 ]\n",
      " ...\n",
      " [3.710705 ]\n",
      " [3.8263974]\n",
      " [2.6416318]]\n",
      "Net(\n",
      "  (hidden): Linear(in_features=16, out_features=50, bias=True)\n",
      "  (hidden2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (predict): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-7.4732, grad_fn=<AddBackward>) tensor(-1.3936, grad_fn=<AddBackward>)\n",
      "15.147189\n",
      "[[-0.1451678 ]\n",
      " [-0.46358576]\n",
      " [-0.00533818]\n",
      " ...\n",
      " [-3.2609704 ]\n",
      " [-3.0264614 ]\n",
      " [-0.00587191]]\n",
      "tensor(0.4183, grad_fn=<AddBackward>) tensor(0.4559, grad_fn=<AddBackward>)\n",
      "1.0398921\n",
      "[[1.7577987]\n",
      " [2.6284647]\n",
      " [1.6337804]\n",
      " ...\n",
      " [3.8787751]\n",
      " [3.836926 ]\n",
      " [2.645694 ]]\n",
      "tensor(0.4331, grad_fn=<AddBackward>) tensor(0.4657, grad_fn=<AddBackward>)\n",
      "1.0134001\n",
      "[[1.774615 ]\n",
      " [2.7509868]\n",
      " [1.7271698]\n",
      " ...\n",
      " [3.8776658]\n",
      " [3.8495371]\n",
      " [2.540949 ]]\n"
     ]
    }
   ],
   "source": [
    "#foldnum = 0\n",
    "for train_index, test_index in kf.split(x):\n",
    "    #ans = np.empty(0)\n",
    "    #pred = np.empty(0)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = torch.from_numpy(X_train)\n",
    "    X_test = torch.from_numpy(X_test)\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    X_train, y_train = Variable(X_train.float()), Variable(y_train.float())\n",
    "    X_test, y_test = Variable(X_test.float()), Variable(y_test.float())\n",
    "\n",
    "    net = Net(n_feature=feature_num, n_hidden=50, n_output=1)     # define the network\n",
    "    print(net)  # net architecture\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    for t in range(600):\n",
    "        y_train_pred = net(X_train)     # input x and predict based on x\n",
    "        \n",
    "        loss = loss_func(y_train_pred, y_train)     # must be (1. nn output, 2. target)\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "        y_test_pred = net(X_test)\n",
    "        y_test_pred = torch.clamp(y_test_pred, min=1, max=5)\n",
    "\n",
    "        y_train_bar = y_train.mean()\n",
    "        sse_train = ((y_train - y_train_pred)**2).sum()\n",
    "        sst_train = ((y_train - y_train_bar)**2).sum()\n",
    "\n",
    "        y_test_bar = y_test.mean()\n",
    "        sse_test = ((y_test - y_test_pred)**2).sum()\n",
    "        sst_test = ((y_test - y_test_bar)**2).sum()\n",
    "\n",
    "        train_r2 = 1 - sse_train/sst_train \n",
    "        test_r2 = 1 - sse_test/sst_test\n",
    "        \n",
    "        if t % 200 == 0:\n",
    "            # print(y_test, y_test_pred)\n",
    "            '''\n",
    "            #draw excel validation answer - prediction scatterplot\n",
    "            y_test_np = y_test.data.numpy()\n",
    "            y_test_pred_np = y_test_pred.data.numpy()\n",
    "            np.savetxt(str(filenum) + \"positive_test.csv\", y_test_np, delimiter=\",\")\n",
    "            np.savetxt(str(filenum) + \"positive_test_pred.csv\", y_test_pred_np, delimiter=\",\")\n",
    "            filenum += 1\n",
    "            \n",
    "            if t == 400:\n",
    "                if ans.size == 0:\n",
    "                    ans = y_test.data.numpy()\n",
    "                else : ans = np.concatenate((ans, y_test.data.numpy()))\n",
    "                if pred.size == 0:\n",
    "                    pred = y_test_pred.data.numpy()\n",
    "                else : pred = np.concatenate((pred, y_test_pred.data.numpy()))\n",
    "            '''\n",
    "            \n",
    "            #np.savetxt(str(filenum) + \"train.txt\", y_train.data.numpy() + y_train_pred.data.numpy())\n",
    "            #filenum += 1\n",
    "            #print(y_test, y_test_pred)\n",
    "            print(train_r2, test_r2)\n",
    "            print(loss.data.numpy())\n",
    "            print(y_train_pred.detach().numpy())\n",
    "            #plt.cla()\n",
    "            #plt.scatter(y_train_pred.data.numpy(), y_train.data.numpy())\n",
    "            #plt.show()\n",
    "            #plt.pause(0.1)\n",
    "    plt.ioff()\n",
    "    #np.savetxt(str(foldnum) + \"test.csv\", ans, delimiter=\",\")\n",
    "    #np.savetxt(str(foldnum) + \"test_pred.csv\",pred, delimiter=\",\")\n",
    "    #foldnum += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
